{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10203472,"sourceType":"datasetVersion","datasetId":6305510}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-tabnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:40:46.803957Z","iopub.execute_input":"2024-12-15T04:40:46.805072Z","iopub.status.idle":"2024-12-15T04:40:56.640370Z","shell.execute_reply.started":"2024-12-15T04:40:46.805032Z","shell.execute_reply":"2024-12-15T04:40:56.639459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:41:32.900515Z","iopub.execute_input":"2024-12-15T04:41:32.900866Z","iopub.status.idle":"2024-12-15T04:41:36.759328Z","shell.execute_reply.started":"2024-12-15T04:41:32.900833Z","shell.execute_reply":"2024-12-15T04:41:36.758637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RowwiseDataset(Dataset):\n    def __init__(self, X_folder, y_attack_folder):\n        self.X_data = pd.concat(\n            [pd.read_parquet(f) for f in sorted(glob.glob(os.path.join(X_folder, \"*.parquet\")))]\n        ).values\n        self.y_attack_data = pd.concat(\n            [pd.read_parquet(f)[\"Attack\"] for f in sorted(glob.glob(os.path.join(y_attack_folder, \"*.parquet\")))]\n        ).values\n        assert len(self.X_data) == len(self.y_attack_data), \\\n            \"Mismatched row counts between features and labels.\"\n        assert self.y_attack_data.ndim == 1, \"Target data should be 1D after column selection.\"\n\n    def __len__(self):\n        return len(self.X_data)\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.X_data[idx], dtype=torch.float32),\n            torch.tensor(self.y_attack_data[idx], dtype=torch.float32),\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:41:55.075329Z","iopub.execute_input":"2024-12-15T04:41:55.076302Z","iopub.status.idle":"2024-12-15T04:41:55.085071Z","shell.execute_reply.started":"2024-12-15T04:41:55.076253Z","shell.execute_reply":"2024-12-15T04:41:55.084132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_folder = \"/kaggle/input/nf-ton-iot-v2-cleaned-split/rmv_outlier_std_801010/X_train\"\ny_train_attack_folder = \"/kaggle/input/nf-ton-iot-v2-cleaned-split/rmv_outlier_std_801010/y_train_attack\"\n\nX_valid_folder = \"/kaggle/input/nf-ton-iot-v2-cleaned-split/rmv_outlier_std_801010/X_valid\"\ny_valid_attack_folder = \"/kaggle/input/nf-ton-iot-v2-cleaned-split/rmv_outlier_std_801010/y_valid_attack\"\n\nX_test_folder = \"/kaggle/input/nf-ton-iot-v2-cleaned-split/rmv_outlier_std_801010/X_test\"\ny_test_attack_folder = \"/kaggle/input/nf-ton-iot-v2-cleaned-split/rmv_outlier_std_801010/y_test_attack\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:41:58.555647Z","iopub.execute_input":"2024-12-15T04:41:58.555979Z","iopub.status.idle":"2024-12-15T04:41:58.560840Z","shell.execute_reply.started":"2024-12-15T04:41:58.555949Z","shell.execute_reply":"2024-12-15T04:41:58.559818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = RowwiseDataset(X_train_folder, y_train_attack_folder)\nvalid_dataset = RowwiseDataset(X_valid_folder, y_valid_attack_folder)\ntest_dataset = RowwiseDataset(X_test_folder, y_test_attack_folder)\n\nX_train, y_train_attack = train_dataset.X_data, train_dataset.y_attack_data\nX_valid, y_valid_attack = valid_dataset.X_data, valid_dataset.y_attack_data\nX_test, y_test_attack = test_dataset.X_data, test_dataset.y_attack_data\n\ny_train = y_train_attack\ny_valid = y_valid_attack\ny_test = y_test_attack","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:42:01.859350Z","iopub.execute_input":"2024-12-15T04:42:01.860042Z","iopub.status.idle":"2024-12-15T04:42:12.146089Z","shell.execute_reply.started":"2024-12-15T04:42:01.860012Z","shell.execute_reply":"2024-12-15T04:42:12.145349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\nprint(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:42:41.687652Z","iopub.execute_input":"2024-12-15T04:42:41.687952Z","iopub.status.idle":"2024-12-15T04:42:41.693085Z","shell.execute_reply.started":"2024-12-15T04:42:41.687927Z","shell.execute_reply":"2024-12-15T04:42:41.691924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WeightedCrossEntropyLoss(nn.Module):\n    def __init__(self, cls_num_list, reduction='mean'):\n        super(WeightedCrossEntropyLoss, self).__init__()\n        total_samples = sum(cls_num_list)\n        self.class_weights = torch.tensor([total_samples / c for c in cls_num_list])\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        log_probs = F.log_softmax(inputs, dim=1)\n        weights = self.class_weights.to(inputs.device)\n        loss = F.nll_loss(log_probs, targets, weight=weights, reduction=self.reduction)\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:42:44.793722Z","iopub.execute_input":"2024-12-15T04:42:44.794438Z","iopub.status.idle":"2024-12-15T04:42:44.799911Z","shell.execute_reply.started":"2024-12-15T04:42:44.794390Z","shell.execute_reply":"2024-12-15T04:42:44.798985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cls_num_list = [4357259, 2943073, 1001429, 961486, 480101, 152251, 125024, 4149, 427, 109]\nweighted_ce_loss = WeightedCrossEntropyLoss(cls_num_list=cls_num_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T04:42:48.998053Z","iopub.execute_input":"2024-12-15T04:42:48.998938Z","iopub.status.idle":"2024-12-15T04:42:49.016282Z","shell.execute_reply.started":"2024-12-15T04:42:48.998903Z","shell.execute_reply":"2024-12-15T04:42:49.015562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sizes = [64]\nresults = []\n\nfor size in sizes:\n    print(f\"Training TabNet with n_a=n_d={size}\")\n\n    clf = TabNetClassifier(\n        device_name='auto',\n        n_d=size,\n        n_a=size,\n        lambda_sparse=0,\n        mask_type='entmax',\n        optimizer_params=dict(lr=1e-2, weight_decay=1e-5),\n        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n        scheduler_params={\n            \"mode\": \"min\",\n            \"factor\": 0.5,\n            \"patience\": 10,\n            \"min_lr\": 1e-5,\n        },\n        verbose=1,\n        seed=42\n    )\n\n    clf.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric=[\"balanced_accuracy\", \"accuracy\"],\n        max_epochs=100000,\n        patience=30,\n        batch_size=1024 * 10,\n        virtual_batch_size=128 * 10,\n        loss_fn=weighted_ce_loss,\n        compute_importance=False,\n    )\n\n    model_path = f\"weighted_cross_entropy_acc_focused_tabnet_model_size_{size}.zip\"\n    clf.save_model(model_path)\n\n    y_pred = clf.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='weighted')\n\n    predictions_path = f\"weighted_cross_acc_focused_entropy_y_pred_size_{size}.csv\"\n    pd.DataFrame(y_pred, columns=[\"y_pred\"]).to_csv(predictions_path, index=False)\n\n    results.append({\n        \"size\": size,\n        \"accuracy\": acc,\n        \"f1_score\": f1,\n        \"model_path\": model_path,\n        \"predictions_path\": predictions_path,\n    })\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"weighted_cross_entropy_acc_focused_results_summary.csv\", index=False)\n\nprint(\"Tuning completed. Models, predictions, and results have been saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}